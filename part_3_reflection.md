For my project, I wanted to analyze two Shakespeare plays. The two plays I chose are The Tragedy of Hamlet and The Tragedy of Macbeth. This is because these are two plays I studied in high school on a literal method, but I never had the opportunity to analyze the text in a more qualitative way. I found the two plays in text from Project Gutenberg and downloaded the two texts onto my resp. From this project, I wanted to see if there are many similarities between the two plays, as they are both tragedy plays written by if not the most famous play writer Shakespeare. 

The first part of my project focuses on importing the plays into my data files in github and performing data cleaning. Next I did text analysis to see same interesting facts about the plays. This includes how many words are in the play, and the number of unique words. I also compared the most common words from the two plays with each other. Lasty, I did a sentiment analysis and fuzz analysis on the play. For the number of unique words, because there were many similarities between the two plays, originally only using the top 10 most frequent was not enough. Because of this, I added the range on the functions to increase the number of most common words considered for the comparison to show adequate result. 

looking at the basic of the text, Hamlet is a longer play compared to Macbeth. This shows as there are 29703 words and 4808 unique words in Hamlet, compared to 17829 words and 3550 unique words in Macbeth.

I think the most interesting finding about the two plays is that there are a lot of common words in both plays. I did expect to find common words, but not the the extend of the top 5 most frequent words for the two plays are the same. The top 10 most common words in hamlet and macbeth are [the, and, to, of, i]. This shows that even though the two plays are different, because they are both writtin by shakespeare, similar writtin style is used with preferences for some words over others. While looking at unique words for each play, the list for Hamlet are [ham, this, but, for] and for Macbeth[be, macb, our, haue]. Of course there is the consideration that we are onlying looking at the 20 most frequent words for each play, and comparing them and finding the ones that do not overlap with the other play. If the range changes to 30 or 40, it will show a different result. For 30, the list combining te two books is['ham', 'lord', 'king', 'macb', 'our', 'all'], so three words each, a decrease from 20 which was four words each. Then when moving to 40, the list increases to 5 words each. A total of 10 words which makes up the list ['ham', 'lord', 'king', 'on', 'if', 'macb', 'thou', 'enter', 'which', 'they']. I think it is very interesting and it further shows just how similar the two plays can be in words. There are only 10 unique words from the two plays from the most frequent 80 words. 

I also performed sentiment analysis for the two plays. The score is {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} for both plays. This is very wierd as it means all the words have a neu rating. 

I think the text import and process, then text analysis part of my project went very well. I am able to successfully process the two plays into text forms onto my respositories, and sucessfully did some text analysis and got qualitative results. However, I do believe I can do better in my word analysis part where I compared the similar and most frequent words. This is because my results are based on the most frequent words from the two books in a given range, and when I tried to look for unique words, it resulted in only a few words, and the list fluctuates depending on the range. In one way this do not align with my expectations as I thought with a bigger range there is going to be more unique words. On the other hand it can also show that my method may be flawed, as the list is also always changing. Next I think my sentiment analysis do not show a solid result. It may be on my part that my code did not successfully run, but from the results I can only tell that the words are neu. without neg and pos or compound. Lastly, I also learned that the fuzz analysis comparing the ratio of the two texts with similarities is not the best choice for this type of analysis. This is because I tried to run the code multiple times, and after many debugging I realized that this method takes a very long time to process, and even waiting over an houor in the end I was not able to get a result. 

